{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7f2dc7",
   "metadata": {
    "id": "0b7f2dc7"
   },
   "source": [
    "<div align=\"center\">\n",
    "<p align=\"center\" style=\"width: 100%;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/vlm-run/.github/refs/heads/main/profile/assets/vlm-black.svg\" alt=\"VLM Run Logo\" width=\"80\" style=\"margin-bottom: -5px; color: #2e3138; vertical-align: middle; padding-right: 5px;\"><br>\n",
    "</p>\n",
    "<p align=\"center\"><a href=\"https://docs.vlm.run\"><b>Website</b></a> | <a href=\"https://docs.vlm.run/\"><b>API Docs</b></a> | <a href=\"https://docs.vlm.run/blog\"><b>Blog</b></a> | <a href=\"https://discord.gg/AMApC2UzVY\"><b>Discord</b></a>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "# VLM Run Orion - 3D Reconstruction\n",
    "\n",
    "This comprehensive cookbook demonstrates [VLM Run Orion's](https://vlm.run/orion) 3D reconstruction capabilities. For more details on the API, see the [Agent API docs](https://docs.vlm.run/agents/introduction).\n",
    "\n",
    "For this notebook, we'll cover how to use the **VLM Run Agent Chat Completions API** - an OpenAI-compatible interface for building powerful 3D reconstruction workflows with the same familiar chat-completions interface.\n",
    "\n",
    "We'll cover the following topics:\n",
    " 1. 3D Reconstruction from Single Images (depth estimation and geometry inference)\n",
    " 2. 3D Reconstruction from Multiple Images (multi-view stereo reconstruction)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.10+\n",
    "- VLM Run API key (get one at [app.vlm.run](https://app.vlm.run))\n",
    "- VLM Run Python Client with OpenAI extra `vlmrun[openai]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b58c5f",
   "metadata": {
    "id": "43b58c5f"
   },
   "source": [
    "## Setup\n",
    "\n",
    "First, install the required packages and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4639c3",
   "metadata": {
    "id": "ae4639c3"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53434b64",
   "metadata": {
    "id": "53434b64"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install vlmrun[openai] --upgrade --quiet\n",
    "%pip install pillow requests numpy opencv-python open3d plydata --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9f8d78",
   "metadata": {
    "id": "fa9f8d78"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import json\n",
    "from typing import List, Any\n",
    "from functools import cached_property\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "VLMRUN_API_KEY = os.getenv(\"VLMRUN_API_KEY\", None)\n",
    "if VLMRUN_API_KEY is None:\n",
    "    VLMRUN_API_KEY = getpass.getpass(\"Enter your VLM Run API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b786ad",
   "metadata": {
    "id": "60b786ad"
   },
   "source": [
    "## Initialize the VLM Run Client\n",
    "\n",
    "We use the OpenAI-compatible chat completions interface through the VLM Run SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf798bbb",
   "metadata": {
    "id": "cf798bbb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764732865860,
     "user_tz": 480,
     "elapsed": 11129,
     "user": {
      "displayName": "Dinesh Reddy Narapureddy",
      "userId": "00411740933014965713"
     }
    },
    "outputId": "367a7a90-92cd-4677-c3f2-b885e933479a"
   },
   "outputs": [],
   "source": [
    "from vlmrun.client import VLMRun\n",
    "\n",
    "client = VLMRun(\n",
    "    api_key=VLMRUN_API_KEY, base_url=\"https://agent.vlm.run/v1\"\n",
    ")\n",
    "print(\"VLM Run client initialized successfully!\")\n",
    "print(f\"Base URL: https://agent.vlm.run/v1\")\n",
    "print(f\"Model: vlmrun-orion-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df2b29e",
   "metadata": {
    "id": "5df2b29e"
   },
   "source": [
    "## Response Models (dtypes)\n",
    "\n",
    "We define Pydantic models for structured outputs. These models include **cached properties** that automatically download and convert images/masks from URLs to PIL Images or numpy arrays for easy manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010d3df2",
   "metadata": {
    "id": "010d3df2"
   },
   "outputs": [],
   "source": [
    "from functools import cached_property\n",
    "from pydantic import BaseModel, Field\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "class Recon3DResponse(BaseModel):\n",
    "    \"\"\"Response model for 3D reconstruction operations, expecting 'recon_path' field.\"\"\"\n",
    "    recon_path: str = Field(..., description=\"Pre-signed URL to the 3D reconstruction file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b565e800",
   "metadata": {
    "id": "b565e800"
   },
   "source": [
    "## Chat Completion Helper\n",
    "\n",
    "We create a helper function to simplify making chat completion requests for 3D reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f477c81b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1764733562410,
     "user": {
      "displayName": "Dinesh Reddy Narapureddy",
      "userId": "00411740933014965713"
     },
     "user_tz": 480
    },
    "id": "f477c81b",
    "outputId": "161186c6-4c45-413c-c837-2bb5a9134f1d"
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import cachetools\n",
    "from typing import Type, TypeVar\n",
    "from IPython.display import HTML\n",
    "from vlmrun.common.image import encode_image\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "def download_ply(url, filename=None):\n",
    "    \"\"\"Download a .ply file and return the local path.\"\"\"\n",
    "    if filename is None:\n",
    "        filename = os.path.basename(url).split(\"?\")[0] or \"model.ply\"\n",
    "    print(f\"Downloading \u2192 {filename}\")\n",
    "    r = requests.get(url, stream=True)\n",
    "    r.raise_for_status()\n",
    "    with open(filename, \"wb\") as f:\n",
    "        for chunk in r.iter_content(8192):\n",
    "            f.write(chunk)\n",
    "    print(f\"Saved to: {filename}\")\n",
    "    return filename\n",
    "\n",
    "\n",
    "def load_gaussian_splat_ply(path):\n",
    "    \"\"\"Load Gaussian splat PLY file and extract parameters.\"\"\"\n",
    "    print(\"Loading Gaussian splat PLY...\")\n",
    "\n",
    "    with open(path, 'rb') as f:\n",
    "        # Read header\n",
    "        line = f.readline().decode('ascii').strip()\n",
    "        if line != 'ply':\n",
    "            raise ValueError(\"Not a valid PLY file\")\n",
    "\n",
    "        line = f.readline().decode('ascii').strip()\n",
    "        if line != 'format binary_little_endian 1.0':\n",
    "            raise ValueError(\"Only binary_little_endian format supported\")\n",
    "\n",
    "        # Read vertex count\n",
    "        line = f.readline().decode('ascii').strip()\n",
    "        if not line.startswith('element vertex '):\n",
    "            raise ValueError(\"Missing vertex count\")\n",
    "        n = int(line.split()[-1])\n",
    "        print(f\"Found {n} Gaussians\")\n",
    "\n",
    "        # Parse properties\n",
    "        fields = {}\n",
    "        idx = 0\n",
    "        while True:\n",
    "            line = f.readline().decode('ascii').strip()\n",
    "            if line == 'end_header':\n",
    "                break\n",
    "            if line.startswith('property float '):\n",
    "                field_name = line.split()[-1]\n",
    "                fields[field_name] = idx\n",
    "                idx += 1\n",
    "\n",
    "        # Required fields\n",
    "        required = ['x', 'y', 'z', 'f_dc_0', 'f_dc_1', 'f_dc_2', 'opacity', 'scale_0', 'scale_1', 'scale_2']\n",
    "        for field in required:\n",
    "            if field not in fields:\n",
    "                raise ValueError(f\"Missing required field: {field}\")\n",
    "\n",
    "        num_fields = len(fields)\n",
    "\n",
    "        # Read binary data\n",
    "        data = np.frombuffer(f.read(), dtype=np.float32)\n",
    "        data = data.reshape(n, num_fields)\n",
    "\n",
    "        # Extract fields\n",
    "        positions = data[:, [fields['x'], fields['y'], fields['z']]].astype(np.float32)\n",
    "        colors = data[:, [fields['f_dc_0'], fields['f_dc_1'], fields['f_dc_2']]].astype(np.float32)\n",
    "        opacities = data[:, fields['opacity']].astype(np.float32)\n",
    "        scales = data[:, [fields['scale_0'], fields['scale_1'], fields['scale_2']]].astype(np.float32)\n",
    "\n",
    "        # Apply transformations\n",
    "        colors = 1 / (1 + np.exp(-colors))  # sigmoid for SH coefficients\n",
    "        opacities = 1 / (1 + np.exp(-opacities))  # sigmoid for opacity\n",
    "        scales = np.exp(scales)  # scales are stored in log space\n",
    "\n",
    "        print(f\"Loaded {n} Gaussians\")\n",
    "        return positions, colors, opacities, scales\n",
    "\n",
    "\n",
    "\n",
    "def render_gaussian_splat(positions, colors, opacities, scales, max_points=100000):\n",
    "    \"\"\"Render Gaussian splat point cloud with soft distance-based sampling.\"\"\"\n",
    "\n",
    "    n = len(positions)\n",
    "\n",
    "    # ---- 1. Compute center ----\n",
    "    center = positions.mean(axis=0)\n",
    "\n",
    "    # ---- 2. Distance from center ----\n",
    "    dist = np.linalg.norm(positions - center, axis=1)\n",
    "\n",
    "    # ---- 3. Soft distance weighting: closer = more weight ----\n",
    "    # sigma = scale of soft falloff (25th percentile of distances)\n",
    "    sigma = np.percentile(dist, 25)\n",
    "    distance_weight = np.exp(-(dist**2) / (2 * sigma**2))\n",
    "\n",
    "    # ---- 4. Soft sampling probability: opacity \u00d7 distance weight ----\n",
    "    probs = distance_weight * opacities\n",
    "    probs /= probs.sum()\n",
    "\n",
    "    # ---- 5. Sample if needed ----\n",
    "    if n > max_points:\n",
    "        print(f\"Soft sampling {max_points} out of {n} points...\")\n",
    "        idx = np.random.choice(n, max_points, replace=False, p=probs)\n",
    "        positions = positions[idx]\n",
    "        colors = colors[idx]\n",
    "        opacities = opacities[idx]\n",
    "        scales = scales[idx]\n",
    "\n",
    "    # ---- 6. Marker sizes ----\n",
    "    scale_mags = np.linalg.norm(scales, axis=1)\n",
    "    sizes = 1 + 9 * (scale_mags - scale_mags.min()) / (scale_mags.max() - scale_mags.min() + 1e-8)\n",
    "\n",
    "    # ---- 7. Color formatting ----\n",
    "    rgb = (colors * 255).astype(np.uint8)\n",
    "    rgba = [f'rgba({r},{g},{b},{a:.3f})'\n",
    "            for r, g, b, a in zip(rgb[:, 0], rgb[:, 1], rgb[:, 2], opacities)]\n",
    "\n",
    "    # ---- 8. Plot ----\n",
    "    fig = go.Figure(data=[go.Scatter3d(\n",
    "        x=positions[:, 0], y=positions[:, 1], z=positions[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(size=sizes, color=rgba, line=dict(width=0), sizemode='diameter')\n",
    "    )])\n",
    "\n",
    "    fig.update_layout(\n",
    "        scene=dict(bgcolor='black', xaxis=dict(visible=False),\n",
    "                   yaxis=dict(visible=False), zaxis=dict(visible=False)),\n",
    "        paper_bgcolor='black', plot_bgcolor='black'\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "T = TypeVar('T', bound=BaseModel)\n",
    "\n",
    "def custom_key(prompt: str, images: list[Image.Image] | list[str] | None = None, response_model: Type[T] | None = None, model: str = \"vlmrun-orion-1:auto\"):\n",
    "    \"\"\"Custom key for caching chat_completion.\"\"\"\n",
    "    image_keys = []\n",
    "    for image in images:\n",
    "        if isinstance(image, Image.Image):\n",
    "            thumb = image.copy()\n",
    "            thumb.thumbnail((128, 128))\n",
    "            encoded = encode_image(thumb, format=\"JPEG\")\n",
    "            image_keys.append(encoded)\n",
    "        elif isinstance(image, str):\n",
    "            image_keys.append(image)\n",
    "\n",
    "\n",
    "    response_key = hashlib.sha256(json.dumps(response_model.model_json_schema(), sort_keys=True).encode()).hexdigest() if response_model else \"\"\n",
    "    return (prompt, tuple(image_keys), response_key, model)\n",
    "\n",
    "\n",
    "\n",
    "#@cachetools.cached(cache=cachetools.TTLCache(maxsize=1000, ttl=3600), key=custom_key)\n",
    "def chat_completion(\n",
    "    prompt: str,\n",
    "    images: list[Image.Image] | list[str] | None = None,\n",
    "    video: str | None = None,\n",
    "    response_model: Type[T] | None = None,\n",
    "    model: str = \"vlmrun-orion-1:auto\"\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Make a chat completion request with optional images and structured output.\n",
    "\n",
    "    Args:\n",
    "        prompt: The text prompt/instruction\n",
    "        images: Optional list of images to process (either PIL Images or URLs)\n",
    "        response_model: Optional Pydantic model for structured output\n",
    "        model: Model to use (default: vlmrun-orion-1:auto)\n",
    "\n",
    "    Returns:\n",
    "        Parsed response model if response_model provided, else raw response text\n",
    "    \"\"\"\n",
    "    content = []\n",
    "    content.append({\"type\": \"text\", \"text\": prompt})\n",
    "\n",
    "    if images:\n",
    "        for image in images:\n",
    "            if isinstance(image, str):\n",
    "                assert image.startswith(\"http\"), \"Image URLs must start with http or https\"\n",
    "                content.append({\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": image, \"detail\": \"auto\"}\n",
    "                })\n",
    "            elif isinstance(image, Image.Image):\n",
    "                content.append({\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": encode_image(image, format=\"JPEG\"), \"detail\": \"auto\"}\n",
    "                })\n",
    "            else:\n",
    "                raise ValueError(\"Images must be either PIL Images or URLs\")\n",
    "    if video:\n",
    "      content.append({\"type\": \"video_url\",\n",
    "                      \"video_url\": {\"url\": video}})\n",
    "\n",
    "    kwargs = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": content}]\n",
    "    }\n",
    "\n",
    "    if response_model:\n",
    "        kwargs[\"response_format\"] = {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"schema\": response_model.model_json_schema()\n",
    "        }\n",
    "\n",
    "    response = client.agent.completions.create(**kwargs)\n",
    "    response_text = response.choices[0].message.content\n",
    "\n",
    "    if response_model:\n",
    "        return response_model.model_validate_json(response_text)\n",
    "\n",
    "    return response_text\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf3f20c",
   "metadata": {
    "id": "2bf3f20c"
   },
   "source": [
    "## 3D Reconstruction Use Cases\n",
    "\n",
    "The VLM Run API can create 3D models from various inputs:\n",
    "1. **From Images** - Single or multiple images of a scene/object\n",
    "2. **From Imagw** - Generate multiple 3D models from text descriptions\n",
    "3. **From Video** - Automatically extract frames and reconstruct the scene"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5579ed",
   "metadata": {
    "id": "cd5579ed"
   },
   "source": [
    "### 1. 3D Reconstruction from a Single Image\n",
    "\n",
    "Create a 3D model from a single image. The model will infer depth and geometry to create a full 3D reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06602e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a06602e2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764728574754,
     "user_tz": 480,
     "elapsed": 189660,
     "user": {
      "displayName": "Dinesh Reddy Narapureddy",
      "userId": "00411740933014965713"
     }
    },
    "outputId": "548f7be3-150b-47dd-dfc4-403773036c2b"
   },
   "outputs": [],
   "source": [
    "IMAGE_URL = \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/agent_use_cases/guided-segmentation/image-11.png\"\n",
    "\n",
    "result = chat_completion(\n",
    "    prompt=f\"Generate a 3D reconstruction of the table in the image\",\n",
    "    images=[IMAGE_URL],\n",
    "    response_model=Recon3DResponse\n",
    ")\n",
    "print(\">> RESPONSE\")\n",
    "print(result)\n",
    "print(\">> IMAGE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from IPython.display import Image\n",
    "Image(url=IMAGE_URL, width=500, height=300)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "9uaKzqqV_IUI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764728578125,
     "user_tz": 480,
     "elapsed": 4,
     "user": {
      "displayName": "Dinesh Reddy Narapureddy",
      "userId": "00411740933014965713"
     }
    },
    "outputId": "8d974e4a-4f97-49cc-b033-b826da07936c"
   },
   "id": "9uaKzqqV_IUI",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PwpQF7nxPLym",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 919,
     "output_embedded_package_id": "1qYwZqAujZrspNq07IDGnoYkay_DZtsGz"
    },
    "executionInfo": {
     "elapsed": 4954,
     "status": "ok",
     "timestamp": 1764728585079,
     "user": {
      "displayName": "Dinesh Reddy Narapureddy",
      "userId": "00411740933014965713"
     },
     "user_tz": 480
    },
    "id": "PwpQF7nxPLym",
    "outputId": "d6daff25-6605-40da-b395-55da9737161e"
   },
   "outputs": [],
   "source": [
    "\n",
    "filename = download_ply(result.recon_path)\n",
    "positions, colors, opacities, scales = load_gaussian_splat_ply(filename)\n",
    "fig = render_gaussian_splat(positions, colors, opacities, scales)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Multi Object 3D Reconstruction from a Single Image\n",
    "\n",
    "Create a multiple 3D model from a single image."
   ],
   "metadata": {
    "id": "KEjMBfpparVq"
   },
   "id": "KEjMBfpparVq"
  },
  {
   "cell_type": "code",
   "source": [
    "IMAGE_URL_FURN=\"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/image.caption/furniture-colorful.jpg\"\n",
    "from IPython.display import Image\n",
    "Image(url=IMAGE_URL_FURN, width=500, height=300)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "8PRmjHer_sGI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764733785376,
     "user_tz": 480,
     "elapsed": 11,
     "user": {
      "displayName": "Dinesh Reddy Narapureddy",
      "userId": "00411740933014965713"
     }
    },
    "outputId": "810dbd00-6855-48a1-ae09-f403366a100e"
   },
   "id": "8PRmjHer_sGI",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "result_multi = chat_completion(\n",
    "    prompt=f\"Generate a 3D reconstruction of the two chairs in the image, by first detecting them, segmenting them and then reconstructing\",\n",
    "    images=[IMAGE_URL_FURN],\n",
    "    response_model=Recon3DResponse\n",
    ")\n",
    "print(\">> RESPONSE\")\n",
    "print(result_multi)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wxVhJAZ0Nsvq",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764734819046,
     "user_tz": 480,
     "elapsed": 286717,
     "user": {
      "displayName": "Dinesh Reddy Narapureddy",
      "userId": "00411740933014965713"
     }
    },
    "outputId": "9534de47-c77e-4b4e-af78-747fa55d8ed5"
   },
   "id": "wxVhJAZ0Nsvq",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649,
     "output_embedded_package_id": "1VDKfoNxVGhQnrwQY1lWcaqoG5ZuwAZdv"
    },
    "executionInfo": {
     "elapsed": 9788,
     "status": "ok",
     "timestamp": 1764735079457,
     "user": {
      "displayName": "Dinesh Reddy Narapureddy",
      "userId": "00411740933014965713"
     },
     "user_tz": 480
    },
    "outputId": "a89b8886-660b-44d6-db69-cd40c3113258",
    "id": "4FI6ukbKBhSy"
   },
   "outputs": [],
   "source": [
    "\n",
    "filename = download_ply(result_multi.recon_path)\n",
    "positions, colors, opacities, scales = load_gaussian_splat_ply(filename)\n",
    "fig = render_gaussian_splat(positions, colors, opacities, scales)\n",
    "fig.show()\n",
    "\n"
   ],
   "id": "4FI6ukbKBhSy"
  },
  {
   "cell_type": "markdown",
   "id": "acf8dbd8",
   "metadata": {
    "id": "acf8dbd8"
   },
   "source": [
    "### 3. 3D Reconstruction from Multiple Images\n",
    "\n",
    "For better results, provide multiple images of the same scene from different viewpoints. This allows the model to better understand depth and geometry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985b2ec3",
   "metadata": {
    "id": "985b2ec3"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "source": [
    "VIDEO_URL = \"https://storage.googleapis.com/vlm-data-public-prod/web/videos/tunnel.mp4\"\n",
    "from IPython.display import Video\n",
    "Video(VIDEO_URL, width=500, height=300)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "0p_b-vzHaXV-",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764735169213,
     "user_tz": 480,
     "elapsed": 33,
     "user": {
      "displayName": "Dinesh Reddy Narapureddy",
      "userId": "00411740933014965713"
     }
    },
    "outputId": "cac1fcce-44db-4548-b2f9-791f4ed66222"
   },
   "id": "0p_b-vzHaXV-",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1YkiYGh3V46",
   "metadata": {
    "id": "f1YkiYGh3V46",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764733123252,
     "user_tz": 480,
     "elapsed": 242204,
     "user": {
      "displayName": "Dinesh Reddy Narapureddy",
      "userId": "00411740933014965713"
     }
    },
    "outputId": "d4bd471a-3234-4b5a-cb0a-0e76cc44f18f"
   },
   "outputs": [],
   "source": [
    "\n",
    "result_scene = chat_completion(\n",
    "    prompt=f\"Generate a 3D reconstruction of the scene but sampling some frames from the video\",\n",
    "    video=VIDEO_URL,\n",
    "    response_model=Recon3DResponse\n",
    ")\n",
    "print(\">> RESPONSE\")\n",
    "print(result_scene)\n",
    "print(\">> IMAGE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "filename = download_ply(result_scene.recon_path)\n",
    "positions, colors, opacities, scales = load_gaussian_splat_ply(filename)\n",
    "fig = render_gaussian_splat(positions, colors, opacities, scales)\n",
    "fig.show()"
   ],
   "metadata": {
    "id": "2OB6io45D-nY",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649,
     "output_embedded_package_id": "1YHB4FVFKIu_6L7TDdWa_h8acLash-LHP"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1764733574488,
     "user_tz": 480,
     "elapsed": 7147,
     "user": {
      "displayName": "Dinesh Reddy Narapureddy",
      "userId": "00411740933014965713"
     }
    },
    "outputId": "9e5d2632-e4c7-49ce-98a0-c40f370ecc9b"
   },
   "id": "2OB6io45D-nY",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28706645"
   },
   "source": [
    "## Conclusion\n",
    "This cookbook demonstrated the comprehensive 3D reconstruction capabilities of the VLM Run Orion Agent API.\n",
    "\n",
    "### Key Takeaways\n",
    "*   **OpenAI-Compatible Interface**: The API follows the OpenAI chat completions format, making it easy to integrate with existing workflows and tools.\n",
    "*   **Structured Outputs**: Use Pydantic models with the `response_model` parameter to get type-safe, validated responses with automatic parsing.\n",
    "*   **3D Reconstruction from Single Images**: Generate 3D models by inferring depth and geometry from a single input image.\n",
    "*   **Multi-Object 3D Reconstruction**: Reconstruct multiple objects within a single image by first detecting and segmenting them.\n",
    "*   **3D Reconstruction from Video**: Utilize videos to automatically extract frames and reconstruct a scene for more robust 3D models.\n",
    "*   **Gaussian Splatting Visualization**: Display 3D reconstruction results using interactive Gaussian Splatting plots.\n",
    "\n",
    "### Next Steps\n",
    "*   Explore the [VLM Run Documentation](https://docs.vlm.run) for more details\n",
    "*   Check out the [Agent API docs](https://docs.vlm.run/agents/introduction) for advanced features\n",
    "*   Join our [Discord community](https://discord.gg/AMApC2UzVY) for support\n",
    "*   Check out more examples in the [VLM Run Cookbook](https://docs.vlm.run/blog)\n",
    "\n",
    "Happy building!"
   ],
   "id": "28706645"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}