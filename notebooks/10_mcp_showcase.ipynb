{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d831c242-afad-4fba-9d14-57d5d918db55",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<p align=\"center\" style=\"width: 100%;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/vlm-run/.github/refs/heads/main/profile/assets/vlm-black.svg\" alt=\"VLM Run Logo\" width=\"80\" style=\"margin-bottom: -5px; color: #2e3138; vertical-align: middle; padding-right: 5px;\"><br>\n",
    "</p>\n",
    "<p align=\"center\"><a href=\"https://docs.vlm.run\"><b>Website</b></a> | <a href=\"https://docs.vlm.run/\"><b>API Docs</b></a> | <a href=\"https://docs.vlm.run/blog\"><b>Blog</b></a> | <a href=\"https://discord.gg/AMApC2UzVY\"><b>Discord</b></a>\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "<a href=\"https://discord.gg/AMApC2UzVY\"><img alt=\"Discord\" src=\"https://img.shields.io/badge/discord-chat-purple?color=%235765F2&label=discord&logo=discord\"></a>\n",
    "<a href=\"https://twitter.com/vlmrun\"><img alt=\"Twitter Follow\" src=\"https://img.shields.io/twitter/follow/vlmrun.svg?style=social&logo=twitter\"></a>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "Welcome to **[VLM Run Cookbooks](https://github.com/vlm-run/vlmrun-cookbook)**, a comprehensive collection of examples and notebooks demonstrating the power of structured visual understanding using the [VLM Run Platform](https://app.vlm.run). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d9ef9d",
   "metadata": {},
   "source": [
    "## 🎨 MCP Showcase\n",
    "\n",
    "This guide walks through our [new MCP server](https://docs.vlm.run/mcp/introduction) and all the tools it provides to date. For a more detailed overview of the MCP server, please refer to the [MCP documentation](https://docs.vlm.run/mcp/introduction) and the [tools reference](https://docs.vlm.run/mcp/tools/overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9766ee-4c06-454a-a111-c83bc0f5305c",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "* Python 3.9+\n",
    "* VLM Run API key (get one at [app.vlm.run](https://app.vlm.run))\n",
    "* OpenAI SDK and API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ae91f55-b3d5-4017-a5da-5cc7a5d386c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install vlmrun openai --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2444835d-5139-4312-a9b5-929abefe5060",
   "metadata": {},
   "source": [
    "## MCP Server Setup\n",
    "\n",
    "First, let's connect to our MCP server and list all the available tools. Our MCP server is remotely hosted on `https://mcp.vlm.run` and currently authenticated using an API key. You can head over to our [API keys](https://app.vlm.run/dashboard/settings/api_keys) page to get your full authenticated MCP server URL.\n",
    "\n",
    "Let's use the OpenAI SDK to connect to our MCP server. In order to test our connection, let's list all the available tools in a neatly formatted markdown table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbf7c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a neatly formatted markdown table listing all the available tools and their descriptions:\n",
      "\n",
      "| Tool Name                                      | Description |\n",
      "|------------------------------------------------|-------------|\n",
      "| put_image_url                                  | Puts an image from a URL into the MCP server and returns an image reference. |\n",
      "| put_file_url                                   | Puts a file from a URL into the MCP server and returns a file reference. |\n",
      "| preview_file_url                               | Gets a preview URL of a file object from an object reference. |\n",
      "| preview_image                                  | Gets a preview image from an image reference. |\n",
      "| parse_image                                    | Parses the image into a structured format, optionally with a prompt. |\n",
      "| rotate_image                                   | Returns a copy of the image rotated by a given angle counterclockwise. |\n",
      "| crop_image                                     | Crops an image to a provided bounding box. |\n",
      "| detect_barcodes                                | Detects barcodes or QR codes in an image. |\n",
      "| get_image_from_index                           | Retrieves an image from a list of images by index. |\n",
      "| document_images                                | Gets/takes images of the pages of a document from a local path. |\n",
      "| detect_document_image_layout                   | Extracts document layout from an image (tables, figures, paragraphs, etc.). |\n",
      "| detect_document_layout                         | Extracts document layout from a document (tables, figures, paragraphs, etc.). |\n",
      "| parse_document                                 | Parses a document file into a structured format (domain-based). |\n",
      "| parse_document_image                           | Parses a document image into a structured format (domain-based). |\n",
      "| search_video                                   | Searches a video for a query and returns relevant scenes, frames, or timestamps. |\n",
      "| transcribe_video                               | Transcribes the entire video and returns detailed audio transcript & scene breakdowns. |\n",
      "| visualize_bboxes                               | Visualizes detected objects in an image (bounding boxes, labels, etc.). |\n",
      "| blur_bboxes                                    | Blurs a set of bounding boxes in an image (for redaction, privacy, etc.). |\n",
      "| detect_faces                                   | Detects faces in an image. |\n",
      "| detect_objects                                 | Detects objects in an image. |\n",
      "| detect_texts                                   | Detects text (OCR) in an image. |\n",
      "| detect_logos                                   | Detects logos in an image. |\n",
      "| get_editable_video                             | Retrieves an editable video object from a file path. |\n",
      "| write_editable_video                           | Writes (saves) a moviepy video object to a file path. |\n",
      "| trim_video                                     | Trims a video to a specific time range. |\n",
      "| add_watermark                                  | Adds a watermark image to a video file. |\n",
      "| add_video_captions                             | Adds automatic subtitles or captions to a video clip. |\n",
      "| find_template                                  | Finds a template image within a larger image using the Grounding DINO model. |\n",
      "| get_hub_info                                   | Gets hub version and information. |\n",
      "| list_domains                                   | Lists available hub domains. |\n",
      "| get_schema                                     | Gets the JSON schema for a specific domain. |\n",
      "| save_schema                                    | Saves JSON schema for a specific domain to the database. |\n",
      "| update_schema                                  | Updates a schema based on a prompt using the SchemaChatInterfaceApplication. |\n",
      "| create_schema                                  | Creates a schema based on a natural language prompt. |\n",
      "\n",
      "Let me know if you would like more details or examples for any specific tool!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", None)\n",
    "if OPENAI_API_KEY is None:\n",
    "    raise ValueError(\"OPENAI_API_KEY is not set\")\n",
    "\n",
    "VLMRUN_API_KEY = os.getenv(\"VLMRUN_API_KEY\", None)\n",
    "if VLMRUN_API_KEY is None:\n",
    "    raise ValueError(\"VLMRUN_API_KEY is not set\")\n",
    "\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = openai.Client(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Fill in the MCP base URL from the API keys page\n",
    "# https://app.vlm.run/dashboard/settings/api_keys\n",
    "MCP_BASE_URL = f\"https://mcp.vlm.run/{VLMRUN_API_KEY}/sse\"\n",
    "\n",
    "# We'll use the OpenAI SDK to connect to our MCP server.\n",
    "result = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"mcp\",\n",
    "            \"server_label\": \"vlmrun-mcp-server\",\n",
    "            \"server_url\": MCP_BASE_URL,\n",
    "            \"require_approval\": \"never\",\n",
    "        },\n",
    "    ],\n",
    "    input=\"List all the available tools in a neatly formatted markdown table\",\n",
    ")\n",
    "print(result.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9560efb2",
   "metadata": {},
   "source": [
    "## Defining a basic LLM agent equipped with [VLM Run MCP](https://docs.vlm.run/mcp/introduction) tools\n",
    "\n",
    "Let's define a basic visual agent workflow that can be used to take in a language instruction input along with image references, and have the LLM agent build a workflow to process the image. The LLM agent will be able to leverage our VLM Run MCP tools in order to accomplish the task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f78fd465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ReasoningStep(BaseModel):\n",
    "    tool_id: str = Field(..., description=\"The tool used in this step\")\n",
    "    reasoning: str = Field(..., description=\"The reasoning for this step\")\n",
    "\n",
    "class ToolCallStep(BaseModel):\n",
    "    tool_id: str = Field(..., description=\"The tool used in this step\")\n",
    "    reasoning: str = Field(..., description=\"A short description of the reasoning for this tool call\")\n",
    "\n",
    "class Response(BaseModel):\n",
    "    reasoning_steps: list[ReasoningStep] = Field(..., description=\"The reasoning steps for the response\")\n",
    "    tool_calls: list[ToolCallStep] = Field(..., description=\"The sequence of tool calls for the response to arrive at the final answer\")\n",
    "    result: str = Field(..., description=\"The final result of the response\")\n",
    "    url: str = Field(..., description=\"The preview URL of the image, in the format https://mcp.vlm.run/files/<file_id>\")\n",
    "\n",
    "def process(prompt: str) -> Response:\n",
    "    \"\"\"The process function is a wrapper around the OpenAI Responses API that \n",
    "    executes multiple tool calls to accomplish the user-intent, and finally \n",
    "    parses the resulting response into a structured format.\"\"\"\n",
    "    result = client.responses.parse(\n",
    "        model=\"gpt-4.1\",\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"mcp\",\n",
    "                \"server_label\": \"vlmrun-mcp-server\",\n",
    "                \"server_url\": MCP_BASE_URL,\n",
    "                \"require_approval\": \"never\",\n",
    "            },\n",
    "        ],\n",
    "        input=prompt,\n",
    "        text_format=Response\n",
    "    )\n",
    "    return result.output_parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11eac6f",
   "metadata": {},
   "source": [
    "## Use-cases\n",
    "\n",
    "Now, let's go ahead and define a few use-cases where the user can provide a language instruction along with image references. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "11cb7548",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Example(BaseModel):\n",
    "    name: str\n",
    "    prompt: str\n",
    "    inputs: dict[str, str]\n",
    "\n",
    "\n",
    "EXAMPLES = [\n",
    "    # image-workflows (basic)\n",
    "    Example(\n",
    "        name=\"load-image-and-preview\",\n",
    "        prompt=\"Load this image ({url}) and preview it.\",\n",
    "        inputs={\"url\": \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/media.tv-news/finance_bb_3_speakers.jpg\"}\n",
    "    ),\n",
    "    Example(\n",
    "        name=\"face-detection\",\n",
    "        prompt=\"Detect the faces in this image ({url}). Overlay the detected faces on the image and preview it.\",\n",
    "        inputs={\"url\": \"https://cdn.mos.cms.futurecdn.net/Yvs83nR9GrDk9bq4Weq5eZ.jpg\"}\n",
    "    ),\n",
    "    Example(\n",
    "        name=\"face-detection-and-blurring\",\n",
    "        prompt=\"Load this image ({url}) and detect all the faces in the image, blur them, and overlay the detected faces on the blurred image, and return the preview URL of the blurred image. \",\n",
    "        inputs={\"url\": \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/media.tv-news/finance_bb_3_speakers.jpg\"}\n",
    "    ),\n",
    "    Example(\n",
    "        name=\"object-detection\",\n",
    "        prompt=\"Load this image ({url}) and detect any objects or items in the image, and return the resulting image preview URL.\",\n",
    "        inputs={\"url\": \"https://github.com/autonomi-ai/nos/blob/main/nos/test/test_data/test.jpg?raw=true\"}\n",
    "    ),\n",
    "    Example(\n",
    "        name=\"qr-code-detection\",\n",
    "        prompt=\"Load this image ({url}) and detect any QR codes in the image, and visualize the bounding box locations of each QR code. Also return the QR code content. \",\n",
    "        inputs={\"url\": \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/image.agent/qr-code-screen.jpg\"}\n",
    "    ),\n",
    "    Example(\n",
    "        name=\"crop-right-face-and-preview\",\n",
    "        prompt=\"Load this image ({url}), detect all the faces in the image, crop the middle face, and preview the cropped face and provide a brief description of the cropped face. \",\n",
    "        inputs={\"url\": \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/media.tv-news/finance_bb_3_speakers.jpg\"}\n",
    "    ),\n",
    "    # image-workflows (advanced)\n",
    "    # Example(\n",
    "    #     name=\"find-template-in-image\",\n",
    "    #     prompt=\"\"\"Given a template image ({template_url}), match it with the following reference image ({url}). Localize the template in the reference image and visualize all the matches with bounding boxes drawn on the reference image. Finally preview the reference image with the bounding boxes. \"\"\",\n",
    "    #     inputs={\n",
    "    #         \"template_url\": \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/image.agent/apple-logo.png\",\n",
    "    #         \"url\": \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/image.agent/reddit-mac-laptops.jpg\"\n",
    "    #     }        \n",
    "    # ),\n",
    "    # document-workflows\n",
    "    Example(\n",
    "        name=\"extract-the-invoice-and-ground-details\",\n",
    "        prompt=\"Load this invoice ({url}), extract only these details: (invoice number, subtotal, total amount, date, etc.). Extract the bounding boxes or grounding information and visualize the invoice with the extracted details. \",\n",
    "        inputs={\"url\": \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/document.invoice/invoice_1.jpg\"}\n",
    "    ),\n",
    "    Example(\n",
    "        name=\"redact-doc-pii\",\n",
    "        prompt=\"Can you redact all the personally identifiable information of the patient (name, address, DOB, phone number, email, etc.) in the following image ({url}) and provide a link to the redacted image.\",\n",
    "        inputs={\"url\": \"https://www.carepatron.com/files/physical-therapy-referral-form-sample-template.jpg\"}\n",
    "    ),\n",
    "    # Example(\n",
    "    #     name=\"document-layout-analysis\",\n",
    "    #     prompt=\"Load this document ({url}), take the first page, detect the layout of the page. Finally visualize the detections on the page image and preview it.\",\n",
    "    #     inputs={\"url\": \"https://storage.googleapis.com/vlm-data-public-prod/hub/examples/document.markdown/2502.13923v1.pdf\"}\n",
    "    # )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4004759",
   "metadata": {},
   "source": [
    "## Let's execute!\n",
    "\n",
    "Now that we have defined the individual use-cases, let's execute it and see how well our LLM agent equipped with the VLM Run MCP tools can accomplish the tasks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2c897ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 8/8 [04:33<00:00, 34.16s/it]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from typing import Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Drop any UserWarning in pydantic\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Define the example with response\n",
    "class ExampleWithResponse(BaseModel):\n",
    "    input: Example\n",
    "    response: Any | None = None\n",
    "    error_msg: str | None = None\n",
    "\n",
    "\n",
    "# Process all the examples\n",
    "responses = []\n",
    "for ex in tqdm(EXAMPLES, desc=\"Processing examples\"):\n",
    "    try:\n",
    "        # Format the prompt with the inputs\n",
    "        ex.prompt = ex.prompt.format(**ex.inputs)\n",
    "        # Process the prompt\n",
    "        response = process(ex.prompt)\n",
    "        # Add the response to the list\n",
    "        responses.append(ExampleWithResponse(input=ex, response=response, error_msg=None))\n",
    "    except Exception as e:\n",
    "        responses.append(ExampleWithResponse(input=ex, response=None, error_msg=str(e)))\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c747c467-24c4-414f-8391-08185f13f816",
   "metadata": {},
   "source": [
    "Let's check the response of all the visual agent workflows we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "62878ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>inputs.url</th>\n",
       "      <th>reasoning_steps</th>\n",
       "      <th>tool_calls</th>\n",
       "      <th>result</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>load-image-and-preview</td>\n",
       "      <td>Load this image (https://storage.googleapis.co...</td>\n",
       "      <td>https://storage.googleapis.com/vlm-data-public...</td>\n",
       "      <td>[{'tool_id': 'mcp_vlmrun-mcp-server.put_image_...</td>\n",
       "      <td>[{'tool_id': 'mcp_vlmrun-mcp-server.put_image_...</td>\n",
       "      <td>The image has been loaded and is available for...</td>\n",
       "      <td>https://mcp.vlm.run/files/img_2c4d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>face-detection</td>\n",
       "      <td>Detect the faces in this image (https://cdn.mo...</td>\n",
       "      <td>https://cdn.mos.cms.futurecdn.net/Yvs83nR9GrDk...</td>\n",
       "      <td>[{'tool_id': 'mcp_vlmrun-mcp-server.put_image_...</td>\n",
       "      <td>[{'tool_id': 'mcp_vlmrun-mcp-server.put_image_...</td>\n",
       "      <td>The detected faces are now overlaid on the ima...</td>\n",
       "      <td>https://mcp.vlm.run/files/img_d1be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>face-detection-and-blurring</td>\n",
       "      <td>Load this image (https://storage.googleapis.co...</td>\n",
       "      <td>https://storage.googleapis.com/vlm-data-public...</td>\n",
       "      <td>[{'tool_id': 'put_image_url', 'reasoning': 'Lo...</td>\n",
       "      <td>[{'tool_id': 'mcp_vlmrun-mcp-server.put_image_...</td>\n",
       "      <td>Here is the preview URL of the image with blur...</td>\n",
       "      <td>https://mcp.vlm.run/files/img_e4ff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>object-detection</td>\n",
       "      <td>Load this image (https://github.com/autonomi-a...</td>\n",
       "      <td>https://github.com/autonomi-ai/nos/blob/main/n...</td>\n",
       "      <td>[{'tool_id': 'put_image_url', 'reasoning': 'Lo...</td>\n",
       "      <td>[{'tool_id': 'mcp_vlmrun-mcp-server.put_image_...</td>\n",
       "      <td>Objects detected in the image include a bench ...</td>\n",
       "      <td>https://mcp.vlm.run/files/img_2561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qr-code-detection</td>\n",
       "      <td>Load this image (https://storage.googleapis.co...</td>\n",
       "      <td>https://storage.googleapis.com/vlm-data-public...</td>\n",
       "      <td>[{'tool_id': 'mcp_vlmrun-mcp-server.put_image_...</td>\n",
       "      <td>[{'tool_id': 'mcp_vlmrun-mcp-server.put_image_...</td>\n",
       "      <td>A QR code was detected in the image. The conte...</td>\n",
       "      <td>https://mcp.vlm.run/files/img_166a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          name  \\\n",
       "0       load-image-and-preview   \n",
       "1               face-detection   \n",
       "2  face-detection-and-blurring   \n",
       "3             object-detection   \n",
       "4            qr-code-detection   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  Load this image (https://storage.googleapis.co...   \n",
       "1  Detect the faces in this image (https://cdn.mo...   \n",
       "2  Load this image (https://storage.googleapis.co...   \n",
       "3  Load this image (https://github.com/autonomi-a...   \n",
       "4  Load this image (https://storage.googleapis.co...   \n",
       "\n",
       "                                          inputs.url  \\\n",
       "0  https://storage.googleapis.com/vlm-data-public...   \n",
       "1  https://cdn.mos.cms.futurecdn.net/Yvs83nR9GrDk...   \n",
       "2  https://storage.googleapis.com/vlm-data-public...   \n",
       "3  https://github.com/autonomi-ai/nos/blob/main/n...   \n",
       "4  https://storage.googleapis.com/vlm-data-public...   \n",
       "\n",
       "                                     reasoning_steps  \\\n",
       "0  [{'tool_id': 'mcp_vlmrun-mcp-server.put_image_...   \n",
       "1  [{'tool_id': 'mcp_vlmrun-mcp-server.put_image_...   \n",
       "2  [{'tool_id': 'put_image_url', 'reasoning': 'Lo...   \n",
       "3  [{'tool_id': 'put_image_url', 'reasoning': 'Lo...   \n",
       "4  [{'tool_id': 'mcp_vlmrun-mcp-server.put_image_...   \n",
       "\n",
       "                                          tool_calls  \\\n",
       "0  [{'tool_id': 'mcp_vlmrun-mcp-server.put_image_...   \n",
       "1  [{'tool_id': 'mcp_vlmrun-mcp-server.put_image_...   \n",
       "2  [{'tool_id': 'mcp_vlmrun-mcp-server.put_image_...   \n",
       "3  [{'tool_id': 'mcp_vlmrun-mcp-server.put_image_...   \n",
       "4  [{'tool_id': 'mcp_vlmrun-mcp-server.put_image_...   \n",
       "\n",
       "                                              result  \\\n",
       "0  The image has been loaded and is available for...   \n",
       "1  The detected faces are now overlaid on the ima...   \n",
       "2  Here is the preview URL of the image with blur...   \n",
       "3  Objects detected in the image include a bench ...   \n",
       "4  A QR code was detected in the image. The conte...   \n",
       "\n",
       "                                  url  \n",
       "0  https://mcp.vlm.run/files/img_2c4d  \n",
       "1  https://mcp.vlm.run/files/img_d1be  \n",
       "2  https://mcp.vlm.run/files/img_e4ff  \n",
       "3  https://mcp.vlm.run/files/img_2561  \n",
       "4  https://mcp.vlm.run/files/img_166a  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([r.model_dump(mode=\"json\") for r in responses])\n",
    "df = pd.concat([pd.json_normalize(df[\"input\"]), pd.json_normalize(df[\"response\"])], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef54a8d8",
   "metadata": {},
   "source": [
    "## Show me the results!\n",
    "\n",
    "Enough talking - let's see the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ee5020f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>result</th>\n",
       "      <th>inputs.url</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>load-image-and-preview</td>\n",
       "      <td><div style=\"width: 300px; word-wrap: break-word;\">Load this image (https://storage.googleapis.com/vlm-data-public-prod/hub/examples/media.tv-news/finance_bb_3_speakers.jpg) and preview it.</div></td>\n",
       "      <td><div style=\"width: 300px; word-wrap: break-word;\">The image has been loaded and is available for preview below.</div></td>\n",
       "      <td><img src=https://storage.googleapis.com/vlm-data-public-prod/hub/examples/media.tv-news/finance_bb_3_speakers.jpg width='500'></td>\n",
       "      <td><img src=https://mcp.vlm.run/files/img_2c4d width='500'></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>face-detection</td>\n",
       "      <td><div style=\"width: 300px; word-wrap: break-word;\">Detect the faces in this image (https://cdn.mos.cms.futurecdn.net/Yvs83nR9GrDk9bq4Weq5eZ.jpg). Overlay the detected faces on the image and preview it.</div></td>\n",
       "      <td><div style=\"width: 300px; word-wrap: break-word;\">The detected faces are now overlaid on the image. You can preview the result using the link below.</div></td>\n",
       "      <td><img src=https://cdn.mos.cms.futurecdn.net/Yvs83nR9GrDk9bq4Weq5eZ.jpg width='500'></td>\n",
       "      <td><img src=https://mcp.vlm.run/files/img_d1be width='500'></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>face-detection-and-blurring</td>\n",
       "      <td><div style=\"width: 300px; word-wrap: break-word;\">Load this image (https://storage.googleapis.com/vlm-data-public-prod/hub/examples/media.tv-news/finance_bb_3_speakers.jpg) and detect all the faces in the image, blur them, and overlay the detected faces on the blurred image, and return the preview URL of the blurred image. </div></td>\n",
       "      <td><div style=\"width: 300px; word-wrap: break-word;\">Here is the preview URL of the image with blurred faces and red rectangles overlayed on the detected faces.</div></td>\n",
       "      <td><img src=https://storage.googleapis.com/vlm-data-public-prod/hub/examples/media.tv-news/finance_bb_3_speakers.jpg width='500'></td>\n",
       "      <td><img src=https://mcp.vlm.run/files/img_e4ff width='500'></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>object-detection</td>\n",
       "      <td><div style=\"width: 300px; word-wrap: break-word;\">Load this image (https://github.com/autonomi-ai/nos/blob/main/nos/test/test_data/test.jpg?raw=true) and detect any objects or items in the image, and return the resulting image preview URL.</div></td>\n",
       "      <td><div style=\"width: 300px; word-wrap: break-word;\">Objects detected in the image include a bench and multiple cars. Here is the processed image with bounding boxes and labels for detected objects.</div></td>\n",
       "      <td><img src=https://github.com/autonomi-ai/nos/blob/main/nos/test/test_data/test.jpg?raw=true width='500'></td>\n",
       "      <td><img src=https://mcp.vlm.run/files/img_2561 width='500'></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qr-code-detection</td>\n",
       "      <td><div style=\"width: 300px; word-wrap: break-word;\">Load this image (https://storage.googleapis.com/vlm-data-public-prod/hub/examples/image.agent/qr-code-screen.jpg) and detect any QR codes in the image, and visualize the bounding box locations of each QR code. Also return the QR code content. </div></td>\n",
       "      <td><div style=\"width: 300px; word-wrap: break-word;\">A QR code was detected in the image. The content is: \"https://vlm.run\". The image with the visualized bounding box (in red) around the QR code is provided at the link below.</div></td>\n",
       "      <td><img src=https://storage.googleapis.com/vlm-data-public-prod/hub/examples/image.agent/qr-code-screen.jpg width='500'></td>\n",
       "      <td><img src=https://mcp.vlm.run/files/img_166a width='500'></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>crop-right-face-and-preview</td>\n",
       "      <td><div style=\"width: 300px; word-wrap: break-word;\">Load this image (https://storage.googleapis.com/vlm-data-public-prod/hub/examples/media.tv-news/finance_bb_3_speakers.jpg), detect all the faces in the image, crop the middle face, and preview the cropped face and provide a brief description of the cropped face. </div></td>\n",
       "      <td><div style=\"width: 300px; word-wrap: break-word;\">The cropped middle face shows a close-up of a woman's face. She has blonde hair, is wearing visible makeup, and her eyes are closed or looking downward with her mouth slightly open, showing some teeth.</div></td>\n",
       "      <td><img src=https://storage.googleapis.com/vlm-data-public-prod/hub/examples/media.tv-news/finance_bb_3_speakers.jpg width='500'></td>\n",
       "      <td><img src=https://mcp.vlm.run/files/img_1f44 width='500'></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>extract-the-invoice-and-ground-details</td>\n",
       "      <td><div style=\"width: 300px; word-wrap: break-word;\">Load this invoice (https://storage.googleapis.com/vlm-data-public-prod/hub/examples/document.invoice/invoice_1.jpg), extract only these details: (invoice number, subtotal, total amount, date, etc.). Extract the bounding boxes or grounding information and visualize the invoice with the extracted details. </div></td>\n",
       "      <td><div style=\"width: 300px; word-wrap: break-word;\">Extracted Invoice Details:\n",
       "- Invoice Number: 9999999\n",
       "- Date: 2023-11-11\n",
       "- Subtotal: 400.00\n",
       "- Total: 400.00\n",
       "\n",
       "The requested fields are visually highlighted in the invoice preview below with their bounding boxes.</div></td>\n",
       "      <td><img src=https://storage.googleapis.com/vlm-data-public-prod/hub/examples/document.invoice/invoice_1.jpg width='500'></td>\n",
       "      <td><img src=https://mcp.vlm.run/files/img_9191 width='500'></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>redact-doc-pii</td>\n",
       "      <td><div style=\"width: 300px; word-wrap: break-word;\">Can you redact all the personally identifiable information of the patient (name, address, DOB, phone number, email, etc.) in the following image (https://www.carepatron.com/files/physical-therapy-referral-form-sample-template.jpg) and provide a link to the redacted image.</div></td>\n",
       "      <td><div style=\"width: 300px; word-wrap: break-word;\">All visible personally identifiable information (PII) such as full names, addresses, birthdate, phone numbers, and email addresses have been redacted from the image. You can view and download the redacted document at the link below.</div></td>\n",
       "      <td><img src=https://www.carepatron.com/files/physical-therapy-referral-form-sample-template.jpg width='500'></td>\n",
       "      <td><img src=https://mcp.vlm.run/files/img_22aa width='500'></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "formatters = {\n",
    "    \"prompt\": lambda x: f'<div style=\"width: 300px; word-wrap: break-word;\">{x}</div>',\n",
    "    \"result\": lambda x: f'<div style=\"width: 300px; word-wrap: break-word;\">{x}</div>',\n",
    "    \"inputs.url\": lambda x: f\"<img src={x} width='500'>\",\n",
    "    \"url\": lambda x: f\"<img src={x} width='500'>\",\n",
    "}\n",
    "\n",
    "cols = [\"name\", \"prompt\", \"result\", \"inputs.url\", \"url\"]\n",
    "html = df[cols].head(n=10).to_html(formatters=formatters, escape=False)\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c48a6b5-1b6a-4886-af8a-0fbc3547ddc5",
   "metadata": {},
   "source": [
    "### 💡 Pro Tips for Using VLM MCP tools\n",
    "\n",
    "📚 Docs: [VLM Run MCP Docs](https://docs.vlm.run/mcp/introduction)\n",
    "🌐 Website: [VLM Run MCP](https://vlm.run/mcp)\n",
    "\n",
    "1. Familiarize yourself with the [tools and the current capabilities](https://docs.vlm.run/mcp/tools/overview)\n",
    "   - Consider working with individual images and fewer tools first to make sure the workflow is working as expected.\n",
    "   - Start building and extending the workflow, one new tool at a time. \n",
    "   - Provide any relevant guidance to the workflow with the tool capability, and avoid explicitly calling out tool-names.\n",
    "\n",
    "2. Best Practices:\n",
    "   - Ensure good image quality\n",
    "   - Validate outputs with structured responses (Pydantic), if possible\n",
    "\n",
    "3. Common Use Cases:\n",
    "   - Document Processing: Invoices, resumes, IDs\n",
    "   - Healthcare: Insurance cards, patient intake forms\n",
    "   - Sports & Media: Game analysis, news content\n",
    "   - Retail: Product cataloging, ad analysis\n",
    "   - Aerospace: Satellite imagery analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
