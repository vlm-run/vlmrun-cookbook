{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df802839-34b5-4169-88fd-73685ce01912",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<p align=\"center\" style=\"width: 100%;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/vlm-run/.github/refs/heads/main/profile/assets/vlm-black.svg\" alt=\"VLM Run Logo\" width=\"80\" style=\"margin-bottom: -5px; color: #2e3138; vertical-align: middle; padding-right: 5px;\"><br>\n",
    "</p>\n",
    "<p align=\"center\"><a href=\"https://vlm.run\"><b>Website</b></a> | <a href=\"https://app.vlm.run/\"><b>Platform</b></a> | <a href=\"https://github.com/vlm-run/vlmrun-hub\"><b>Hub</b></a> | <a href=\"https://docs.vlm.run/\"><b>Docs</b></a> | <a href=\"https://vlm.run/blog\"><b>Blog</b></a> | <a href=\"https://discord.gg/4jgyECY4rq\"><b>Discord</b></a>\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "<a href=\"https://discord.gg/4jgyECY4rq\"><img alt=\"Discord\" src=\"https://img.shields.io/badge/discord-chat-purple?color=%235765F2&label=discord&logo=discord\"></a>\n",
    "<a href=\"https://twitter.com/vlmrun\"><img alt=\"Twitter Follow\" src=\"https://img.shields.io/badge/twitter-follow-blue?color=%231DA1F2&label=twitter&logo=twitter\"></a>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "This notebook demonstrates how to use the **VLM Run MCP Server** with OpenAI Responses API. The VLM Run MCP Server provides access to powerful visual AI capabilities through the Model Context Protocol (MCP).\n",
    "\n",
    "## What is VLM Run MCP Server?\n",
    "\n",
    "The VLM Run MCP Server transforms any MCP-compatible AI agent into a visual AI powerhouse. It provides access to:\n",
    "\n",
    "- **Document AI**: Extract structured data from invoices, receipts, contracts, forms\n",
    "- **Image AI**: Classify images, extract text, analyze visual content, detect objects and faces\n",
    "- **Video AI**: Transcribe videos with scene descriptions, search content, edit videos\n",
    "- **Hub Management**: Browse 50+ pre-built domains and create custom schemas\n",
    "\n",
    "**Server URL**: `https://mcp.vlm.run/mcp/sse`  \n",
    "**Authentication**: Bearer token (VLM Run API key)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.9+\n",
    "- VLM Run API key (get one at [app.vlm.run](https://app.vlm.run))\n",
    "- OpenAI API key (for OpenAI examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8194644-eead-4193-90dd-ffbaf8dccafe",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d9c7bc7-b4df-429b-a925-9e6a8be3b092",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install openai --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5711959-0eaa-41d9-ba51-3b2b0659de4e",
   "metadata": {},
   "source": [
    "### 1. OpenAI API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d5c9f7e-7e47-40ca-8568-e45f04118d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.Client(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870b43d1-3841-41bf-9817-b4386d0849c7",
   "metadata": {},
   "source": [
    "### 2. Image Processing Examples\n",
    "\n",
    "#### Face Detection and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcf46435-fa9b-434b-b039-c54ad4b5cdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face Detection Result:\n",
      "Here is the visualized image with detected faces: [View Image](https://mcp.vlm.run/files/img_7136)\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[{\n",
    "        \"type\": \"mcp\",\n",
    "        \"server_label\": \"vlm-run-mcp\",\n",
    "        \"server_url\": \"https://mcp.vlm.run/mcp/sse\",\n",
    "        \"require_approval\": \"never\"\n",
    "    }],\n",
    "    input=\"Load this image (https://storage.googleapis.com/vlm-data-public-prod/hub/examples/media.tv-news/finance_bb_3_speakers.jpg) and detect all the faces in the image, visualize the detected faces, and return the preview URL of the visualized image.\"\n",
    ")\n",
    "\n",
    "print(\"Face Detection Result:\")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980b62d3-d59d-4fde-a82c-f86c0dbe8e21",
   "metadata": {},
   "source": [
    "#### Face Blurring and Privacy Protection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20f7c52f-58d2-4f0f-97b1-d8fefefbd14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face Blurring Result:\n",
      "The faces have been detected and blurred in the image. You can preview the result [here](https://mcp.vlm.run/files/img_3108).\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[{\n",
    "        \"type\": \"mcp\",\n",
    "        \"server_label\": \"vlm-run-mcp\",\n",
    "        \"server_url\": \"https://mcp.vlm.run/mcp/sse\",\n",
    "        \"require_approval\": \"never\"\n",
    "    }],\n",
    "    input=\"Load this image (https://storage.googleapis.com/vlm-data-public-prod/hub/examples/media.tv-news/finance_bb_3_speakers.jpg) and detect all the faces in the image, blur them, and overlay the detected faces on the blurred image, and return the preview URL of the blurred image.\"\n",
    ")\n",
    "\n",
    "print(\"Face Blurring Result:\")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd14ac4-d2fd-4ff5-a517-6fdadf81aace",
   "metadata": {},
   "source": [
    "### 3. Document Processing Examples\n",
    "\n",
    "#### Invoice Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44d69ca9-68b5-44e2-815c-fd5a427c1a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoice Extraction Result:\n",
      "Here are the extracted details from the invoice including grounding information:\n",
      "\n",
      "**Issuer:**\n",
      "- Name: Google\n",
      "- Bounding Box: [0.06196, 0.01515, 0.25333, 0.06727]\n",
      "\n",
      "**Invoice Number:**\n",
      "- Number: 23413561D\n",
      "- Bounding Box: [0.81020, 0.07152, 0.12392, 0.01636]\n",
      "\n",
      "**Invoice Dates:**\n",
      "- Issue Date: Sep 24, 2019\n",
      "  - Bounding Box: [0.82745, 0.13818, 0.09961, 0.01576]\n",
      "- Due Date: Sep 30, 2019\n",
      "  - Bounding Box: [0.82745, 0.16606, 0.09961, 0.01576]\n",
      "\n",
      "**Customer:**\n",
      "- Name: Jane Smith\n",
      "- Bounding Box: [0.08078, 0.16364, 0.09255, 0.01455]\n",
      "- Address: 1600 Amphitheatre Pkway, Mountain View, CA 94043\n",
      "  - Bounding Box (Street): [0.08157, 0.17879, 0.19059, 0.01576]\n",
      "  - Bounding Box (City, State, Zip): [0.08, 0.19455, 0.19137, 0.01455]\n",
      "\n",
      "**Items:**\n",
      "1. **12 ft HDMI cable**\n",
      "   - Quantity: 12\n",
      "   - Unit Price: $9.99\n",
      "   - Total Price: $119.88\n",
      "   - Bounding Box (Total Price): [0.86588, 0.30545, 0.06275, 0.01394]\n",
      "   \n",
      "2. **27\" Computer Monitor**\n",
      "   - Quantity: 12\n",
      "   - Unit Price: $399.99\n",
      "   - Total Price: $4,799.88\n",
      "   - Bounding Box (Total Price): [0.85333, 0.32727, 0.07451, 0.01394]\n",
      "   \n",
      "3. **Ergonomic Keyboard**\n",
      "   - Quantity: 12\n",
      "   - Unit Price: $59.99\n",
      "   - Total Price: $719.88\n",
      "   - Bounding Box (Total Price): [0.86588, 0.34970, 0.06275, 0.01394]\n",
      "   \n",
      "4. **Optical mouse**\n",
      "   - Quantity: 12\n",
      "   - Unit Price: $19.99\n",
      "   - Total Price: $239.88\n",
      "   - Bounding Box (Total Price): [0.86588, 0.37212, 0.06275, 0.01394]\n",
      "   \n",
      "5. **Laptop**\n",
      "   - Quantity: 12\n",
      "   - Unit Price: $1,299.99\n",
      "   - Total Price: $15,599.88\n",
      "   - Bounding Box (Total Price): [0.84392, 0.39455, 0.08471, 0.01394]\n",
      "   \n",
      "6. **Misc processing fees**\n",
      "   - Quantity: 1\n",
      "   - Unit Price: $899.99\n",
      "   - Total Price: $899.99\n",
      "   - Bounding Box (Total Price): [0.86588, 0.41636, 0.06196, 0.01394]\n",
      "\n",
      "**Totals:**\n",
      "- Subtotal: $22,379.39\n",
      "  - Bounding Box: [0.84392, 0.49939, 0.08471, 0.01455]\n",
      "- Tax: $1,767.97\n",
      "  - Bounding Box: [0.85333, 0.55576, 0.07529, 0.01455]\n",
      "- Total: $19,647.68\n",
      "  - Bounding Box: [0.84392, 0.61152, 0.08471, 0.01455]\n",
      "\n",
      "**Notes:**\n",
      "- \"This is a test order. No actual transactions took place.\"\n",
      "- Bounding Box: [0.07922, 0.73091, 0.38980, 0.01576]\n",
      "\n",
      "If you have further questions or need more details, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[{\n",
    "        \"type\": \"mcp\",\n",
    "        \"server_label\": \"vlm-run-mcp\",\n",
    "        \"server_url\": \"https://mcp.vlm.run/mcp/sse\",\n",
    "        \"require_approval\": \"never\"\n",
    "    }],\n",
    "    input=\"Load this invoice (https://storage.googleapis.com/vlm-data-public-prod/hub/examples/document.invoice/google_invoice.pdf) and extract the invoice details (invoice number, total amount, date, etc.) including its grounding information.\"\n",
    ")\n",
    "\n",
    "print(\"Invoice Extraction Result:\")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe14789-ad0f-4e73-a605-beb34abbb646",
   "metadata": {},
   "source": [
    "#### PII Redaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deead439-41ca-4318-87b9-4797b151a95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PII Redaction Result:\n",
      "The personal information has been redacted from the document. You can view the redacted version [here](https://mcp.vlm.run/files/img_10e3).\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[{\n",
    "        \"type\": \"mcp\",\n",
    "        \"server_label\": \"vlm-run-mcp\",\n",
    "        \"server_url\": \"https://mcp.vlm.run/mcp/sse\",\n",
    "        \"require_approval\": \"never\"\n",
    "    }],\n",
    "    input=\"Load this document (https://storage.googleapis.com/vlm-data-public-prod/hub/examples/document.invoice/invoice_1.jpg) and redact any personal information (name, address, phone number, email, etc.) from the document.\"\n",
    ")\n",
    "\n",
    "print(\"PII Redaction Result:\")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b507666-1794-4eee-8dae-3769babec304",
   "metadata": {},
   "source": [
    "### 4. Hub Schema Management\n",
    "\n",
    "#### List Available Domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cb89eae-2a71-4166-bfcd-cf9c70be3797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Domains:\n",
      "Here are the first 5 available hub domains and their categories:\n",
      "\n",
      "1. **accounting.form-payslip**\n",
      "   - **Category**: Accounting\n",
      "\n",
      "2. **accounting.form-w2**\n",
      "   - **Category**: Accounting\n",
      "\n",
      "3. **aerospace.remote-sensing**\n",
      "   - **Category**: Aerospace\n",
      "\n",
      "4. **document.bank-check**\n",
      "   - **Category**: Document\n",
      "\n",
      "5. **document.bank-statement**\n",
      "   - **Category**: Document\n",
      "\n",
      "These schemas cover a wide range of domains and categories.\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[{\n",
    "        \"type\": \"mcp\",\n",
    "        \"server_label\": \"vlm-run-mcp\",\n",
    "        \"server_url\": \"https://mcp.vlm.run/mcp/sse\",\n",
    "        \"require_approval\": \"never\"\n",
    "    }],\n",
    "    input=\"List all available hub domains and show me the first 5 domains with their categories. I want to see what schemas are available.\"\n",
    ")\n",
    "\n",
    "print(\"Available Domains:\")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2f0e70-42a9-448a-82be-e580876d3d20",
   "metadata": {},
   "source": [
    "#### Get Schema Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7da91978-63eb-4dbd-9f8e-0aee2d6411a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoice Schema:\n",
      "The 'document.invoice' domain schema is designed for extracting and structuring invoice data. Here's a breakdown of its structure and available fields:\n",
      "\n",
      "### Schema Description\n",
      "A comprehensive system for extracting structured information from invoice images, covering invoice metadata, customer details, line items, and financial totals.\n",
      "\n",
      "### Fields\n",
      "\n",
      "- **invoice_id**: Unique identifier for the invoice.\n",
      "- **period_start**: Start date of the invoice period.\n",
      "- **period_end**: End date of the invoice period.\n",
      "- **invoice_issue_date**: Date the invoice was issued.\n",
      "- **invoice_due_date**: Date the invoice is due.\n",
      "- **order_id**: Unique identifier for the order.\n",
      "- **customer_id**: Unique identifier for the customer.\n",
      "- **issuer**: Name of the issuer of the invoice.\n",
      "- **issuer_address**:\n",
      "  - street\n",
      "  - city\n",
      "  - state\n",
      "  - postal_code\n",
      "  - country\n",
      "- **customer**: Name of the recipient.\n",
      "- **customer_email**: Email of the recipient.\n",
      "- **customer_phone**: Phone number of the recipient.\n",
      "- **customer_billing_address**:\n",
      "  - street\n",
      "  - city\n",
      "  - state\n",
      "  - postal_code\n",
      "  - country\n",
      "- **customer_shipping_address**:\n",
      "  - street\n",
      "  - city\n",
      "  - state\n",
      "  - postal_code\n",
      "  - country\n",
      "- **items**: Array of items in the invoice, each with:\n",
      "  - description\n",
      "  - quantity\n",
      "  - currency\n",
      "  - unit_price\n",
      "  - total_price\n",
      "- **subtotal**: Subtotal amount of the invoice.\n",
      "- **tax**: Tax amount of the invoice.\n",
      "- **total**: Total amount of the invoice.\n",
      "- **currency**: Currency of the invoice.\n",
      "- **notes**: Additional notes on the invoice.\n",
      "\n",
      "This schema aids in capturing various aspects of an invoice in a structured format for easy access and analysis.\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[{\n",
    "        \"type\": \"mcp\",\n",
    "        \"server_label\": \"vlm-run-mcp\",\n",
    "        \"server_url\": \"https://mcp.vlm.run/mcp/sse\",\n",
    "        \"require_approval\": \"never\"\n",
    "    }],\n",
    "    input=\"Get the JSON schema for the 'document.invoice' domain. Show me the schema structure and available fields.\"\n",
    ")\n",
    "\n",
    "print(\"Invoice Schema:\")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a4996f-c3fe-44a4-a2c3-3b337bb323df",
   "metadata": {},
   "source": [
    "### 5. Advanced Examples\n",
    "\n",
    "#### Face Cropping and Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f654679d-1f6d-4498-89ec-cad983bfbed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face Cropping Result:\n",
      "Here is the cropped image of the right-most face from the original image:\n",
      "\n",
      "![Cropped Face](https://mcp.vlm.run/files/img_6f7a)\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[{\n",
    "        \"type\": \"mcp\",\n",
    "        \"server_label\": \"vlm-run-mcp\",\n",
    "        \"server_url\": \"https://mcp.vlm.run/mcp/sse\",\n",
    "        \"require_approval\": \"never\"\n",
    "    }],\n",
    "    input=\"Load this image (https://storage.googleapis.com/vlm-data-public-prod/hub/examples/media.tv-news/finance_bb_3_speakers.jpg), detect all the faces in the image, and crop the right-most face, and preview the cropped image in HTML.\"\n",
    ")\n",
    "\n",
    "print(\"Face Cropping Result:\")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a88b02-7176-4b80-8b45-ee10e2bd89ca",
   "metadata": {},
   "source": [
    "#### Video Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece3cd1f-6ace-4abd-be62-c55c99559b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[{\n",
    "        \"type\": \"mcp\",\n",
    "        \"server_label\": \"vlm-run-mcp\",\n",
    "        \"server_url\": \"https://mcp.vlm.run/mcp/sse\",\n",
    "        \"require_approval\": \"never\"\n",
    "    }],\n",
    "    input=\"Load this video (https://storage.googleapis.com/vlm-data-public-prod/hub/examples/video/test_video.mp4) and preview the video in HTML.\"\n",
    ")\n",
    "\n",
    "print(\"Video Preview:\")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46140e8e-dc54-4d4c-b566-fc0525ad7653",
   "metadata": {},
   "source": [
    "### 7. Next Steps and Resources\n",
    "\n",
    "#### Explore More Capabilities\n",
    "\n",
    "1. **Advanced Video Processing**: Try video transcription, editing, and analysis\n",
    "2. **Batch Processing**: Process large volumes of documents efficiently\n",
    "3. **Real-time Applications**: Build live video processing applications\n",
    "4. **Multi-modal Analysis**: Combine text, image, and video analysis\n",
    "\n",
    "#### Tool Categories\n",
    "\n",
    "- **Image Processing**: Object detection, OCR, face detection, QR code detection, template matching\n",
    "- **Document Processing**: Invoice extraction, PII redaction, figure detection\n",
    "- **Video Processing**: Video preview, trimming, watermarking, YouTube transcript extraction\n",
    "- **Hub Management**: Schema listing, creation, updating, and filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8652779-5432-4516-a789-d3122a105157",
   "metadata": {},
   "source": [
    "#### Resources\n",
    "\n",
    "- [VLM Run MCP Documentation](https://docs.vlm.run/mcp)\n",
    "- [FastMCP Documentation](https://gofastmcp.com)\n",
    "- [VLM Run Hub](https://github.com/vlm-run/vlmrun-hub)\n",
    "- [Discord Community](https://discord.gg/4jgyECY4rq)\n",
    "\n",
    "#### Support\n",
    "\n",
    "- Email: [support@vlm.run](mailto:support@vlm.run)\n",
    "- Discord: [Join our community](https://discord.gg/4jgyECY4rq)\n",
    "- GitHub: [Report issues](https://github.com/vlm-run/vlmrun-hub/issues)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
